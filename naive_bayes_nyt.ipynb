{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "import re\n",
    "import string\n",
    "\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import make_pipeline\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem.porter import PorterStemmer\n",
    "\n",
    "from sklearn.naive_bayes import MultinomialNB\n",
    "\n",
    "from sklearn.metrics import accuracy_score"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "array([0.1, 0.2, 0.3, 0.4, 0.5, 0.6, 0.7, 0.8, 0.9])"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "np.arange(0.1, 1, 0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comments File\n",
    "comments = 'Data/CommentsApril2018.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ml/lib/python3.6/site-packages/IPython/core/interactiveshell.py:3018: DtypeWarning: Columns (32) have mixed types. Specify dtype option on import or set low_memory=False.\n",
      "  interactivity=interactivity, compiler=compiler, result=result)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approveDate</th>\n",
       "      <th>articleID</th>\n",
       "      <th>articleWordCount</th>\n",
       "      <th>commentBody</th>\n",
       "      <th>commentID</th>\n",
       "      <th>commentSequence</th>\n",
       "      <th>commentTitle</th>\n",
       "      <th>commentType</th>\n",
       "      <th>createDate</th>\n",
       "      <th>depth</th>\n",
       "      <th>...</th>\n",
       "      <th>status</th>\n",
       "      <th>timespeople</th>\n",
       "      <th>trusted</th>\n",
       "      <th>typeOfMaterial</th>\n",
       "      <th>updateDate</th>\n",
       "      <th>userDisplayName</th>\n",
       "      <th>userID</th>\n",
       "      <th>userLocation</th>\n",
       "      <th>userTitle</th>\n",
       "      <th>userURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1524594282</td>\n",
       "      <td>5adf6684068401528a2aa69b</td>\n",
       "      <td>781.0</td>\n",
       "      <td>How could the league possibly refuse this offe...</td>\n",
       "      <td>26853969.0</td>\n",
       "      <td>26853969.0</td>\n",
       "      <td>&lt;br/&gt;</td>\n",
       "      <td>comment</td>\n",
       "      <td>1524594011</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>News</td>\n",
       "      <td>1524594282</td>\n",
       "      <td>Christopher Rillo</td>\n",
       "      <td>46566740.0</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1524594252</td>\n",
       "      <td>5adf6684068401528a2aa69b</td>\n",
       "      <td>781.0</td>\n",
       "      <td>So then the execs can be like \"yeah...we will ...</td>\n",
       "      <td>26853699.0</td>\n",
       "      <td>26853699.0</td>\n",
       "      <td>&lt;br/&gt;</td>\n",
       "      <td>comment</td>\n",
       "      <td>1524593146</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>News</td>\n",
       "      <td>1524594252</td>\n",
       "      <td>Matt Brand</td>\n",
       "      <td>64324866.0</td>\n",
       "      <td>Williamsburg, Brooklyn</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1524594250</td>\n",
       "      <td>5adf6684068401528a2aa69b</td>\n",
       "      <td>781.0</td>\n",
       "      <td>I would not want to play chess against these c...</td>\n",
       "      <td>26853677.0</td>\n",
       "      <td>26853677.0</td>\n",
       "      <td>&lt;br/&gt;</td>\n",
       "      <td>comment</td>\n",
       "      <td>1524593032</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>News</td>\n",
       "      <td>1524594250</td>\n",
       "      <td>Joseph</td>\n",
       "      <td>78105093.0</td>\n",
       "      <td>Fayetteville, AR</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1524593431</td>\n",
       "      <td>5adf6684068401528a2aa69b</td>\n",
       "      <td>781.0</td>\n",
       "      <td>Could the cheerleaders join the Actors' Equity...</td>\n",
       "      <td>26853784.0</td>\n",
       "      <td>26853784.0</td>\n",
       "      <td>&lt;br/&gt;</td>\n",
       "      <td>comment</td>\n",
       "      <td>1524593426</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>News</td>\n",
       "      <td>1524593431</td>\n",
       "      <td>Stephen</td>\n",
       "      <td>81939618.0</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1524595048</td>\n",
       "      <td>5adf653f068401528a2aa697</td>\n",
       "      <td>656.0</td>\n",
       "      <td>Seeking conclusions which support preconceived...</td>\n",
       "      <td>26854236.0</td>\n",
       "      <td>26854236.0</td>\n",
       "      <td>&lt;br/&gt;</td>\n",
       "      <td>comment</td>\n",
       "      <td>1524595043</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>News</td>\n",
       "      <td>1524595048</td>\n",
       "      <td>Paul Zorsky</td>\n",
       "      <td>58642997.0</td>\n",
       "      <td>Texas</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   approveDate                 articleID  articleWordCount  \\\n",
       "0   1524594282  5adf6684068401528a2aa69b             781.0   \n",
       "1   1524594252  5adf6684068401528a2aa69b             781.0   \n",
       "2   1524594250  5adf6684068401528a2aa69b             781.0   \n",
       "3   1524593431  5adf6684068401528a2aa69b             781.0   \n",
       "4   1524595048  5adf653f068401528a2aa697             656.0   \n",
       "\n",
       "                                         commentBody   commentID  \\\n",
       "0  How could the league possibly refuse this offe...  26853969.0   \n",
       "1  So then the execs can be like \"yeah...we will ...  26853699.0   \n",
       "2  I would not want to play chess against these c...  26853677.0   \n",
       "3  Could the cheerleaders join the Actors' Equity...  26853784.0   \n",
       "4  Seeking conclusions which support preconceived...  26854236.0   \n",
       "\n",
       "   commentSequence commentTitle commentType  createDate  depth   ...     \\\n",
       "0       26853969.0        <br/>     comment  1524594011    1.0   ...      \n",
       "1       26853699.0        <br/>     comment  1524593146    1.0   ...      \n",
       "2       26853677.0        <br/>     comment  1524593032    1.0   ...      \n",
       "3       26853784.0        <br/>     comment  1524593426    1.0   ...      \n",
       "4       26854236.0        <br/>     comment  1524595043    1.0   ...      \n",
       "\n",
       "     status  timespeople trusted  typeOfMaterial  updateDate  \\\n",
       "0  approved            1       0            News  1524594282   \n",
       "1  approved            1       0            News  1524594252   \n",
       "2  approved            1       0            News  1524594250   \n",
       "3  approved            0       0            News  1524593431   \n",
       "4  approved            1       0            News  1524595048   \n",
       "\n",
       "     userDisplayName      userID            userLocation  userTitle  userURL  \n",
       "0  Christopher Rillo  46566740.0           San Francisco        NaN      NaN  \n",
       "1         Matt Brand  64324866.0  Williamsburg, Brooklyn        NaN      NaN  \n",
       "2             Joseph  78105093.0        Fayetteville, AR        NaN      NaN  \n",
       "3            Stephen  81939618.0             Phoenix, AZ        NaN      NaN  \n",
       "4        Paul Zorsky  58642997.0                   Texas        NaN      NaN  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read in File\n",
    "comm = pd.read_csv(comments); comm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['approveDate', 'articleID', 'articleWordCount', 'commentBody',\n",
       "       'commentID', 'commentSequence', 'commentTitle', 'commentType',\n",
       "       'createDate', 'depth', 'editorsSelection', 'inReplyTo', 'newDesk',\n",
       "       'parentID', 'parentUserDisplayName', 'permID', 'picURL', 'printPage',\n",
       "       'recommendations', 'recommendedFlag', 'replyCount', 'reportAbuseFlag',\n",
       "       'sectionName', 'sharing', 'status', 'timespeople', 'trusted',\n",
       "       'typeOfMaterial', 'updateDate', 'userDisplayName', 'userID',\n",
       "       'userLocation', 'userTitle', 'userURL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comm.columns"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "def vec_train_test(data, labels, vectorizer, algorithm, random_state=28):\n",
    "    # Create train/test split with labels\n",
    "    train_data, test_data, train_target, test_target = train_test_split(data,\n",
    "                                                                        labels,\n",
    "                                                                        random_state=random_state)\n",
    "    \n",
    "    model = make_pipeline(vectorizer, algorithm)\n",
    "    model.fit(train_data, train_target)\n",
    "    predicted = model.predict(test_data)\n",
    "    return accuracy_score(predicted, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "All Comments\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = comm.commentBody\n",
    "sections = comm.sectionName"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "random_state = 28"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbModel = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6418293548338392"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vec_train_test(data, sections, vectorizer, nbModel, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6359408736090351"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vec_train_test(data, sections, vectorizer, nbModel, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6076761637299753"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(input='content',\n",
    "                             stop_words='english',\n",
    "                             ngram_range=(1,2))\n",
    "vec_train_test(data, sections, vectorizer, nbModel, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5936042034696744"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(input='content',\n",
    "                             stop_words='english',\n",
    "                             ngram_range=(1,3))\n",
    "vec_train_test(data, sections, vectorizer, nbModel, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5884857544050369"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vec_train_test(data, sections, vectorizer, nbModel, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5957633132521025"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "vec_train_test(data, sections, vectorizer, nbModel, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5613081487520949"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(input='content', stop_words='english', ngram_range=(1,2))\n",
    "vec_train_test(data, sections, vectorizer, nbModel, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5601002551675197"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(input='content', stop_words='english', ngram_range=(1,3))\n",
    "vec_train_test(data, sections, vectorizer, nbModel, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize text and return a non-unique list of tokenized words\n",
    "    found in the text. Normalize to lowercase, strip punctuation,\n",
    "    remove stop words, drop words of length < 3, strip digits.\n",
    "    \"\"\"\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", text)  # delete stuff but leave at least a space to avoid clumping together\n",
    "    words = nopunct.split(\" \")\n",
    "    words = [w for w in words if len(w) > 2]  # ignore a, an, to, at, be, ...\n",
    "    words = [w.lower() for w in words]\n",
    "    goodwords = [w for w in words if w not in ENGLISH_STOP_WORDS]\n",
    "    return goodwords\n",
    "\n",
    "\n",
    "def stemwords(words):\n",
    "    \"\"\"\n",
    "    Given a list of tokens/words, return a new list with each word\n",
    "    stemmed using a PorterStemmer.\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in words]\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    return stemwords(tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(input = 'content',\n",
    "                            analyzer = 'word',\n",
    "                            tokenizer = tokenizer,\n",
    "                            stop_words = 'english',\n",
    "                            decode_error = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5954613398559587"
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_train_test(data, sections, vectorizer, nbModel, random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remove Unknown Category\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = comm.sectionName != \"Unknown\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = comm.commentBody[filt]\n",
    "sections = comm.sectionName[filt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "nbModel = MultinomialNB()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6808814527647419"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vec_train_test(data, sections, vectorizer, nbModel, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.6664626266748283"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer()\n",
    "vec_train_test(data, sections, vectorizer, nbModel, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5884173297966402"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(input='content',\n",
    "                             stop_words='english',\n",
    "                             ngram_range=(1,2))\n",
    "vec_train_test(data, sections, vectorizer, nbModel, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5703257838536353"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(input='content',\n",
    "                             stop_words='english',\n",
    "                             ngram_range=(1,3))\n",
    "vec_train_test(data, sections, vectorizer, nbModel, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5306400054410665"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer()\n",
    "vec_train_test(data, sections, vectorizer, nbModel, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5509759912942936"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(stop_words='english')\n",
    "vec_train_test(data, sections, vectorizer, nbModel, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.4998979800040808"
      ]
     },
     "execution_count": 30,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(input='content', stop_words='english', ngram_range=(1,2))\n",
    "vec_train_test(data, sections, vectorizer, nbModel, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.498129633408148"
      ]
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(input='content', stop_words='english', ngram_range=(1,3))\n",
    "vec_train_test(data, sections, vectorizer, nbModel, random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(input = 'content',\n",
    "                            analyzer = 'word',\n",
    "                            tokenizer = tokenizer,\n",
    "                            stop_words = 'english',\n",
    "                            decode_error = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {
    "scrolled": true
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.5560429844249473"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vec_train_test(data, sections, vectorizer, nbModel, random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter to Comments Only without Unknowns\n",
    "----------"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = (comm.sectionName != \"Unknown\") & (comm.commentType == \"comment\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = comm.commentBody[filt]\n",
    "sections = comm.sectionName[filt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.708839422933915"
      ]
     },
     "execution_count": 36,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vec_train_test(data, sections, vectorizer, nbModel, random_state)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Using Linear Support Vector Machines \n",
    "----------"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Filter to Comments without Unknowns  \n",
    "https://towardsdatascience.com/multi-class-text-classification-with-scikit-learn-12f1e60e0a9f  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = (comm.sectionName != \"Unknown\") & (comm.commentType == \"comment\")\n",
    "data = comm.commentBody[filt]\n",
    "sections = comm.sectionName[filt]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.7045263001338555"
      ]
     },
     "execution_count": 39,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = CountVectorizer(stop_words='english')\n",
    "vec_train_test(data, sections, vectorizer, LinearSVC(), random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7549452183828268"
      ]
     },
     "execution_count": 40,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer = TfidfVectorizer(input = 'content',\n",
    "                            analyzer = 'word',\n",
    "                            tokenizer = tokenizer,\n",
    "                            stop_words = 'english',\n",
    "                            decode_error = 'ignore')\n",
    "vec_train_test(data, sections, vectorizer, LinearSVC(), random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(input = 'content',\n",
    "                            analyzer = 'word',\n",
    "                            tokenizer = tokenizer,\n",
    "                            stop_words = 'english',\n",
    "                            decode_error = 'ignore')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_target, test_target = train_test_split(data, sections, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "<60511x38172 sparse matrix of type '<class 'numpy.float64'>'\n",
       "\twith 1652602 stored elements in Compressed Sparse Row format>"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "vectorizer.fit_transform(train_data)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.base import BaseEstimator\n",
    "# from sklearn.feature_extraction.text import VectorizerMixin\n",
    "from sklearn.pipeline import Pipeline\n",
    "\n",
    "class DummyEstimator(BaseEstimator):\n",
    "    def fit(self): pass\n",
    "    def score(self): pass\n",
    "    \n",
    "# class DummyVectorizer(VectorizerMixin):\n",
    "#     def fit(self): pass\n",
    "#     def transform(self): pass\n",
    "    \n",
    "# Create a pipeline\n",
    "pipe = Pipeline([('clf', DummyEstimator())]) # Placeholder Estimator"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.linear_model import LogisticRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "search_space = [#{\n",
    "#     'vec': [CountVectorizer(input='content', decode_error='ignore', analyzer='word')],\n",
    "#     'vec__tokenizer': [None, tokenizer],\n",
    "#     'vec__stop_words': [None, 'english'],\n",
    "#     'vec__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
    "#     'vec__max_df': np.arange(0.1, 1, 0.1),\n",
    "#     'vec__min_df': np.arange(0.1, 1, 0.1)},\n",
    "    \n",
    "#     {'vec': [TfidfVectorizer(input='content', decode_error='ignore', analyzer='word')],\n",
    "#     'vec__tokenizer': [None, tokenizer],\n",
    "#     'vec__stop_words': [None, 'english'],\n",
    "#     'vec__ngram_range': [(1, 1), (1, 2), (1, 3)]},\n",
    "#     'vec__max_df': np.arange(0.1, 1, 0.1),\n",
    "#     'vec__min_df': np.arange(0.1, 1, 0.1)},\n",
    "    \n",
    "    {'clf': [LogisticRegression(random_state=random_state)],\n",
    "    'clf__penalty': ['l1', 'l2'],\n",
    "    'clf__C': np.logspace(0, 4, 10),\n",
    "    'clf__fit_intercept': [False, True],\n",
    "    'clf__solver': ['newton-cg', 'lbfgs', 'liblinear', 'sag', 'saga'],\n",
    "    'clf__multi_class': ['ovr', 'multinomial', 'auto']}]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(pipe, search_space, cv=5, verbose=0)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_target, test_target = train_test_split(data, sections, random_state=random_state)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "ename": "TypeError",
     "evalue": "Singleton array array(TfidfVectorizer(analyzer='word', binary=False, decode_error='ignore',\n        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n        stop_words='english', strip_accents=None, sublinear_tf=False,\n        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n        tokenizer=<function tokenizer at 0x1a16c8f730>, use_idf=True,\n        vocabulary=None), dtype=object) cannot be considered a valid collection.",
     "output_type": "error",
     "traceback": [
      "\u001b[0;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[0;31mTypeError\u001b[0m                                 Traceback (most recent call last)",
      "\u001b[0;32m<ipython-input-32-0ff6188281b9>\u001b[0m in \u001b[0;36m<module>\u001b[0;34m\u001b[0m\n\u001b[0;32m----> 1\u001b[0;31m \u001b[0mbest_model\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mgs\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mfit\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mvectorizer\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mtrain_target\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m",
      "\u001b[0;32m/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/model_selection/_search.py\u001b[0m in \u001b[0;36mfit\u001b[0;34m(self, X, y, groups, **fit_params)\u001b[0m\n\u001b[1;32m    672\u001b[0m             \u001b[0mrefit_metric\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m'score'\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    673\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 674\u001b[0;31m         \u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mindexable\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    675\u001b[0m         \u001b[0mn_splits\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mcv\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mget_n_splits\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0my\u001b[0m\u001b[0;34m,\u001b[0m \u001b[0mgroups\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    676\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mindexable\u001b[0;34m(*iterables)\u001b[0m\n\u001b[1;32m    253\u001b[0m         \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    254\u001b[0m             \u001b[0mresult\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mappend\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0marray\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 255\u001b[0;31m     \u001b[0mcheck_consistent_length\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0;34m*\u001b[0m\u001b[0mresult\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    256\u001b[0m     \u001b[0;32mreturn\u001b[0m \u001b[0mresult\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    257\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36mcheck_consistent_length\u001b[0;34m(*arrays)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \"\"\"\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m<listcomp>\u001b[0;34m(.0)\u001b[0m\n\u001b[1;32m    224\u001b[0m     \"\"\"\n\u001b[1;32m    225\u001b[0m \u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0;32m--> 226\u001b[0;31m     \u001b[0mlengths\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0;34m[\u001b[0m\u001b[0m_num_samples\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mX\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;32mfor\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32min\u001b[0m \u001b[0marrays\u001b[0m \u001b[0;32mif\u001b[0m \u001b[0mX\u001b[0m \u001b[0;32mis\u001b[0m \u001b[0;32mnot\u001b[0m \u001b[0;32mNone\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[0m\u001b[1;32m    227\u001b[0m     \u001b[0muniques\u001b[0m \u001b[0;34m=\u001b[0m \u001b[0mnp\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0munique\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mlengths\u001b[0m\u001b[0;34m)\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    228\u001b[0m     \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0muniques\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m>\u001b[0m \u001b[0;36m1\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;32m/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/utils/validation.py\u001b[0m in \u001b[0;36m_num_samples\u001b[0;34m(x)\u001b[0m\n\u001b[1;32m    140\u001b[0m         \u001b[0;32mif\u001b[0m \u001b[0mlen\u001b[0m\u001b[0;34m(\u001b[0m\u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m)\u001b[0m \u001b[0;34m==\u001b[0m \u001b[0;36m0\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    141\u001b[0m             raise TypeError(\"Singleton array %r cannot be considered\"\n\u001b[0;32m--> 142\u001b[0;31m                             \" a valid collection.\" % x)\n\u001b[0m\u001b[1;32m    143\u001b[0m         \u001b[0;32mreturn\u001b[0m \u001b[0mx\u001b[0m\u001b[0;34m.\u001b[0m\u001b[0mshape\u001b[0m\u001b[0;34m[\u001b[0m\u001b[0;36m0\u001b[0m\u001b[0;34m]\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n\u001b[1;32m    144\u001b[0m     \u001b[0;32melse\u001b[0m\u001b[0;34m:\u001b[0m\u001b[0;34m\u001b[0m\u001b[0;34m\u001b[0m\u001b[0m\n",
      "\u001b[0;31mTypeError\u001b[0m: Singleton array array(TfidfVectorizer(analyzer='word', binary=False, decode_error='ignore',\n        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n        stop_words='english', strip_accents=None, sublinear_tf=False,\n        token_pattern='(?u)\\\\b\\\\w\\\\w+\\\\b',\n        tokenizer=<function tokenizer at 0x1a16c8f730>, use_idf=True,\n        vocabulary=None), dtype=object) cannot be considered a valid collection."
     ]
    }
   ],
   "source": [
    "best_model = gs.fit(vectorizer.tranform(, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Create space of candidate learning algorithms and their hyperparameters\n",
    "search_space = [{'clf': [LogisticRegression()], # Actual Estimator\n",
    "                 'clf__penalty': ['l1', 'l2'],\n",
    "                 'clf__C': np.logspace(0, 4, 10)},\n",
    "                \n",
    "                {'clf': [DecisionTreeClassifier()],  # Actual Estimator\n",
    "                 'clf__criterion': ['gini', 'entropy']}]\n",
    "\n",
    "\n",
    "# Create grid search \n",
    "clf = GridSearchCV(pipe, search_space, cv=5, verbose=0)\n",
    "\n",
    "#  Fit grid search\n",
    "best_model = clf.fit(X_train, y_train);\n",
    "\n",
    "# View best model\n",
    "best_model.best_estimator_.get_params()['clf']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV\n",
    "\n",
    "pipe = Pipeline([('vec', StandardScaler()),          # Transformer: Standardize\n",
    "                ('pca', PCA(n_components=2)),       # Transformer: Dimension Reduction\n",
    "                ('clf', DecisionTreeClassifier())]) # Estimator: ML algorithm\n",
    "\n",
    "# Grid search over each element in pipeline\n",
    "#                  <estimator>__<parameter>                            \n",
    "grid_params = dict(pca__n_components=[1, 2, 3],\n",
    "                   clf__max_depth=range(1, 5),\n",
    "                   clf__criterion=['gini', 'entropy'],\n",
    "                   clf__min_samples_leaf =range(3, 15))\n",
    "\n",
    "gs = GridSearchCV(estimator=pipe_dt,  \n",
    "                  param_grid=grid_params,\n",
    "                  scoring='accuracy',\n",
    "                  cv=10)\n",
    "\n",
    "gs.fit(X_train, y_train)\n",
    "f\"{gs.score(X_test, y_test):.4f}\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.7"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
