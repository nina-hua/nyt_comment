{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "reset -fs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from sklearn.feature_extraction.text import CountVectorizer\n",
    "from sklearn.metrics import accuracy_score\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.pipeline import Pipeline\n",
    "from sklearn.svm import LinearSVC"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "RANDOM_STATE = 28"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Load Data and Filter"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Comments File\n",
    "comments = 'Data/CommentsApril2018.csv'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Read in File\n",
    "comm = pd.read_csv(comments)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Index(['approveDate', 'articleID', 'articleWordCount', 'commentBody',\n",
       "       'commentID', 'commentSequence', 'commentTitle', 'commentType',\n",
       "       'createDate', 'depth', 'editorsSelection', 'inReplyTo', 'newDesk',\n",
       "       'parentID', 'parentUserDisplayName', 'permID', 'picURL', 'printPage',\n",
       "       'recommendations', 'recommendedFlag', 'replyCount', 'reportAbuseFlag',\n",
       "       'sectionName', 'sharing', 'status', 'timespeople', 'trusted',\n",
       "       'typeOfMaterial', 'updateDate', 'userDisplayName', 'userID',\n",
       "       'userLocation', 'userTitle', 'userURL'],\n",
       "      dtype='object')"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comm.columns"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Column \"userTitle\" has both NaN and string data types. Replace Nan with \"Unknown\" to have one uniform data type in the column"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>approveDate</th>\n",
       "      <th>articleID</th>\n",
       "      <th>articleWordCount</th>\n",
       "      <th>commentBody</th>\n",
       "      <th>commentID</th>\n",
       "      <th>commentSequence</th>\n",
       "      <th>commentTitle</th>\n",
       "      <th>commentType</th>\n",
       "      <th>createDate</th>\n",
       "      <th>depth</th>\n",
       "      <th>...</th>\n",
       "      <th>status</th>\n",
       "      <th>timespeople</th>\n",
       "      <th>trusted</th>\n",
       "      <th>typeOfMaterial</th>\n",
       "      <th>updateDate</th>\n",
       "      <th>userDisplayName</th>\n",
       "      <th>userID</th>\n",
       "      <th>userLocation</th>\n",
       "      <th>userTitle</th>\n",
       "      <th>userURL</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1524594282</td>\n",
       "      <td>5adf6684068401528a2aa69b</td>\n",
       "      <td>781.0</td>\n",
       "      <td>How could the league possibly refuse this offe...</td>\n",
       "      <td>26853969.0</td>\n",
       "      <td>26853969.0</td>\n",
       "      <td>&lt;br/&gt;</td>\n",
       "      <td>comment</td>\n",
       "      <td>1524594011</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>News</td>\n",
       "      <td>1524594282</td>\n",
       "      <td>Christopher Rillo</td>\n",
       "      <td>46566740.0</td>\n",
       "      <td>San Francisco</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1524594252</td>\n",
       "      <td>5adf6684068401528a2aa69b</td>\n",
       "      <td>781.0</td>\n",
       "      <td>So then the execs can be like \"yeah...we will ...</td>\n",
       "      <td>26853699.0</td>\n",
       "      <td>26853699.0</td>\n",
       "      <td>&lt;br/&gt;</td>\n",
       "      <td>comment</td>\n",
       "      <td>1524593146</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>News</td>\n",
       "      <td>1524594252</td>\n",
       "      <td>Matt Brand</td>\n",
       "      <td>64324866.0</td>\n",
       "      <td>Williamsburg, Brooklyn</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1524594250</td>\n",
       "      <td>5adf6684068401528a2aa69b</td>\n",
       "      <td>781.0</td>\n",
       "      <td>I would not want to play chess against these c...</td>\n",
       "      <td>26853677.0</td>\n",
       "      <td>26853677.0</td>\n",
       "      <td>&lt;br/&gt;</td>\n",
       "      <td>comment</td>\n",
       "      <td>1524593032</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>News</td>\n",
       "      <td>1524594250</td>\n",
       "      <td>Joseph</td>\n",
       "      <td>78105093.0</td>\n",
       "      <td>Fayetteville, AR</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1524593431</td>\n",
       "      <td>5adf6684068401528a2aa69b</td>\n",
       "      <td>781.0</td>\n",
       "      <td>Could the cheerleaders join the Actors' Equity...</td>\n",
       "      <td>26853784.0</td>\n",
       "      <td>26853784.0</td>\n",
       "      <td>&lt;br/&gt;</td>\n",
       "      <td>comment</td>\n",
       "      <td>1524593426</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>0</td>\n",
       "      <td>0</td>\n",
       "      <td>News</td>\n",
       "      <td>1524593431</td>\n",
       "      <td>Stephen</td>\n",
       "      <td>81939618.0</td>\n",
       "      <td>Phoenix, AZ</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1524595048</td>\n",
       "      <td>5adf653f068401528a2aa697</td>\n",
       "      <td>656.0</td>\n",
       "      <td>Seeking conclusions which support preconceived...</td>\n",
       "      <td>26854236.0</td>\n",
       "      <td>26854236.0</td>\n",
       "      <td>&lt;br/&gt;</td>\n",
       "      <td>comment</td>\n",
       "      <td>1524595043</td>\n",
       "      <td>1.0</td>\n",
       "      <td>...</td>\n",
       "      <td>approved</td>\n",
       "      <td>1</td>\n",
       "      <td>0</td>\n",
       "      <td>News</td>\n",
       "      <td>1524595048</td>\n",
       "      <td>Paul Zorsky</td>\n",
       "      <td>58642997.0</td>\n",
       "      <td>Texas</td>\n",
       "      <td>Unknown</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 34 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "   approveDate                 articleID  articleWordCount  \\\n",
       "0   1524594282  5adf6684068401528a2aa69b             781.0   \n",
       "1   1524594252  5adf6684068401528a2aa69b             781.0   \n",
       "2   1524594250  5adf6684068401528a2aa69b             781.0   \n",
       "3   1524593431  5adf6684068401528a2aa69b             781.0   \n",
       "4   1524595048  5adf653f068401528a2aa697             656.0   \n",
       "\n",
       "                                         commentBody   commentID  \\\n",
       "0  How could the league possibly refuse this offe...  26853969.0   \n",
       "1  So then the execs can be like \"yeah...we will ...  26853699.0   \n",
       "2  I would not want to play chess against these c...  26853677.0   \n",
       "3  Could the cheerleaders join the Actors' Equity...  26853784.0   \n",
       "4  Seeking conclusions which support preconceived...  26854236.0   \n",
       "\n",
       "   commentSequence commentTitle commentType  createDate  depth   ...     \\\n",
       "0       26853969.0        <br/>     comment  1524594011    1.0   ...      \n",
       "1       26853699.0        <br/>     comment  1524593146    1.0   ...      \n",
       "2       26853677.0        <br/>     comment  1524593032    1.0   ...      \n",
       "3       26853784.0        <br/>     comment  1524593426    1.0   ...      \n",
       "4       26854236.0        <br/>     comment  1524595043    1.0   ...      \n",
       "\n",
       "     status  timespeople trusted  typeOfMaterial  updateDate  \\\n",
       "0  approved            1       0            News  1524594282   \n",
       "1  approved            1       0            News  1524594252   \n",
       "2  approved            1       0            News  1524594250   \n",
       "3  approved            0       0            News  1524593431   \n",
       "4  approved            1       0            News  1524595048   \n",
       "\n",
       "     userDisplayName      userID            userLocation  userTitle  userURL  \n",
       "0  Christopher Rillo  46566740.0           San Francisco    Unknown      NaN  \n",
       "1         Matt Brand  64324866.0  Williamsburg, Brooklyn    Unknown      NaN  \n",
       "2             Joseph  78105093.0        Fayetteville, AR    Unknown      NaN  \n",
       "3            Stephen  81939618.0             Phoenix, AZ    Unknown      NaN  \n",
       "4        Paul Zorsky  58642997.0                   Texas    Unknown      NaN  \n",
       "\n",
       "[5 rows x 34 columns]"
      ]
     },
     "execution_count": 25,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comm.iloc[:, 32] = comm.iloc[:, 32].replace(np.nan, 'Unknown', regex=True); comm.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "filt = (comm.sectionName != \"Unknown\") & (comm.commentType == \"comment\")\n",
    "data = comm.commentBody[filt]\n",
    "sections = comm.sectionName[filt]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Train/Test Split and Training"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "train_data, test_data, train_target, test_target = train_test_split(data, sections, random_state=RANDOM_STATE)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = CountVectorizer(decode_error='ignore',\n",
    "                             stop_words='english')\n",
    "lsvc_model = Pipeline([('vec', vectorizer),\n",
    "                       ('clf', LinearSVC(random_state=RANDOM_STATE))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='ignore',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        s... max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=28, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc_model.fit(train_data, train_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Predictions and Metric Evaluation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7046254523821328"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predictions = lsvc_model.predict(test_data)\n",
    "accuracy_score(predictions, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Grid Search"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.model_selection import GridSearchCV"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params = dict(vec__ngram_range=[(1,1), (1,2), (1,3)],\n",
    "                   clf__loss=['hinge', 'squared_hinge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs = GridSearchCV(estimator=lsvc_model,\n",
    "                 param_grid=grid_params,\n",
    "                 scoring='accuracy',\n",
    "                 cv=5,\n",
    "                 n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/model_selection/_split.py:626: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='ignore',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), preprocessor=None, stop_words='english',\n",
       "        s... max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=28, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'vec__ngram_range': [(1, 1), (1, 2), (1, 3)], 'clf__loss': ['hinge', 'squared_hinge']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.fit(train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__loss': 'squared_hinge', 'vec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vec', CountVectorizer(analyzer='word', binary=False, decode_error='ignore',\n",
       "        dtype=<class 'numpy.int64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), preprocessor=None, stop_words='english',\n",
       "        s... max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=28, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 17,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7359079867135987"
      ]
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs.score(test_data, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## With TdidfVectorizor"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "metadata": {},
   "outputs": [],
   "source": [
    "import re\n",
    "import string\n",
    "from sklearn.feature_extraction.text import TfidfVectorizer\n",
    "from sklearn.feature_extraction.text import ENGLISH_STOP_WORDS\n",
    "from nltk.stem.porter import PorterStemmer"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "metadata": {},
   "outputs": [],
   "source": [
    "def tokenize(text):\n",
    "    \"\"\"\n",
    "    Tokenize text and return a non-unique list of tokenized words\n",
    "    found in the text. Normalize to lowercase, strip punctuation,\n",
    "    remove stop words, drop words of length < 3, strip digits.\n",
    "    \"\"\"\n",
    "    regex = re.compile('[' + re.escape(string.punctuation) + '0-9\\\\r\\\\t\\\\n]')\n",
    "    nopunct = regex.sub(\" \", text)  # delete stuff but leave at least a space to avoid clumping together\n",
    "    words = nopunct.split(\" \")\n",
    "    words = [w for w in words if len(w) > 2]  # ignore a, an, to, at, be, ...\n",
    "    words = [w.lower() for w in words]\n",
    "    goodwords = [w for w in words if w not in ENGLISH_STOP_WORDS]\n",
    "    return goodwords\n",
    "\n",
    "\n",
    "def stemwords(words):\n",
    "    \"\"\"\n",
    "    Given a list of tokens/words, return a new list with each word\n",
    "    stemmed using a PorterStemmer.\n",
    "    \"\"\"\n",
    "    stemmer = PorterStemmer()\n",
    "    return [stemmer.stem(w) for w in words]\n",
    "\n",
    "\n",
    "def tokenizer(text):\n",
    "    return stemwords(tokenize(text))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(input='content',\n",
    "                            analyzer='word',\n",
    "#                             tokenizer=tokenizer,\n",
    "                            stop_words='english',\n",
    "                            decode_error='ignore')\n",
    "\n",
    "lsvc_model = Pipeline([('vec', vectorizer),\n",
    "                       ('clf', LinearSVC(random_state=RANDOM_STATE))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='ignore',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ... max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=28, tol=0.0001,\n",
       "     verbose=0))])"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc_model.fit(train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.7550443706311041"
      ]
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "predicted = lsvc_model.predict(test_data)\n",
    "accuracy_score(predicted, test_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [],
   "source": [
    "grid_params = dict(vec__ngram_range=[(1,1), (1,2), (1,3)],\n",
    "                   clf__loss=['hinge', 'squared_hinge'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [],
   "source": [
    "gs2 = GridSearchCV(estimator=lsvc_model,\n",
    "                 param_grid=grid_params,\n",
    "                 scoring='accuracy',\n",
    "                 cv=5,\n",
    "                 n_jobs=-1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/model_selection/_split.py:626: Warning: The least populated class in y has only 1 members, which is too few. The minimum number of members in any class cannot be less than n_splits=5.\n",
      "  % (min_groups, self.n_splits)), Warning)\n",
      "/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "GridSearchCV(cv=5, error_score='raise-deprecating',\n",
       "       estimator=Pipeline(memory=None,\n",
       "     steps=[('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='ignore',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ... max_iter=1000,\n",
       "     multi_class='ovr', penalty='l2', random_state=28, tol=0.0001,\n",
       "     verbose=0))]),\n",
       "       fit_params=None, iid='warn', n_jobs=-1,\n",
       "       param_grid={'vec__ngram_range': [(1, 1), (1, 2), (1, 3)], 'clf__loss': ['hinge', 'squared_hinge']},\n",
       "       pre_dispatch='2*n_jobs', refit=True, return_train_score='warn',\n",
       "       scoring='accuracy', verbose=0)"
      ]
     },
     "execution_count": 26,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2.fit(train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "{'clf__loss': 'hinge', 'vec__ngram_range': (1, 2)}"
      ]
     },
     "execution_count": 27,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2.best_params_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='ignore',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 2), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...nge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=28, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2.best_estimator_"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "0.769718903376134"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "gs2.score(test_data, test_target)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Optimized Model with Classification Report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.metrics import classification_report"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [],
   "source": [
    "vectorizer = TfidfVectorizer(input='content',\n",
    "                            analyzer='word',\n",
    "#                             tokenizer=tokenizer,\n",
    "                            stop_words='english',\n",
    "                            decode_error='ignore')\n",
    "\n",
    "lsvc_model = Pipeline([('vec', vectorizer),\n",
    "                       ('clf', LinearSVC(random_state=RANDOM_STATE, loss='hinge'))])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/svm/base.py:922: ConvergenceWarning: Liblinear failed to converge, increase the number of iterations.\n",
      "  \"the number of iterations.\", ConvergenceWarning)\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "Pipeline(memory=None,\n",
       "     steps=[('vec', TfidfVectorizer(analyzer='word', binary=False, decode_error='ignore',\n",
       "        dtype=<class 'numpy.float64'>, encoding='utf-8', input='content',\n",
       "        lowercase=True, max_df=1.0, max_features=None, min_df=1,\n",
       "        ngram_range=(1, 1), norm='l2', preprocessor=None, smooth_idf=True,\n",
       " ...nge', max_iter=1000, multi_class='ovr',\n",
       "     penalty='l2', random_state=28, tol=0.0001, verbose=0))])"
      ]
     },
     "execution_count": 53,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "lsvc_model.fit(train_data, train_target)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 54,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "                        precision    recall  f1-score   support\n",
      "\n",
      "                Africa       0.95      0.50      0.66        76\n",
      "              Americas       0.77      0.44      0.56       232\n",
      "          Art & Design       0.87      0.42      0.57        80\n",
      "          Asia Pacific       0.69      0.55      0.61       943\n",
      "             Australia       1.00      0.39      0.56        23\n",
      "           Book Review       0.62      0.07      0.13       180\n",
      "                Canada       0.77      0.68      0.72       133\n",
      "    College Basketball       1.00      0.50      0.67        18\n",
      "               Cycling       0.95      0.71      0.82        28\n",
      "              DealBook       0.59      0.27      0.37        73\n",
      "                   Eat       0.91      0.70      0.79       119\n",
      "               Economy       0.69      0.44      0.54       294\n",
      "                Europe       0.67      0.39      0.49       733\n",
      "                Family       0.73      0.60      0.66       327\n",
      "                  Golf       1.00      0.67      0.80        12\n",
      "                Hockey       0.71      0.42      0.53        12\n",
      "              Learning       1.00      0.24      0.38        17\n",
      "          Lesson Plans       0.00      0.00      0.00         2\n",
      "                  Live       0.88      0.66      0.75       163\n",
      "                 Media       0.84      0.64      0.73       924\n",
      "           Middle East       0.76      0.68      0.72      1490\n",
      "                  Mind       0.80      0.67      0.73         6\n",
      "                  Move       0.86      0.63      0.73        98\n",
      "                 Music       0.80      0.50      0.62        48\n",
      "         Personal Tech       0.54      0.41      0.47        73\n",
      "              Politics       0.77      0.93      0.84     10420\n",
      "          Pro Football       0.78      0.64      0.70        11\n",
      "         Sunday Review       0.72      0.64      0.68      3099\n",
      "            Television       0.90      0.64      0.75       492\n",
      "             The Daily       0.00      0.00      0.00        11\n",
      "              Weddings       0.80      0.50      0.62        16\n",
      "Wine, Beer & Cocktails       0.85      0.61      0.71        18\n",
      "\n",
      "             micro avg       0.76      0.76      0.76     20171\n",
      "             macro avg       0.76      0.50      0.59     20171\n",
      "          weighted avg       0.76      0.76      0.75     20171\n",
      "\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/metrics/classification.py:1143: UndefinedMetricWarning: Precision and F-score are ill-defined and being set to 0.0 in labels with no predicted samples.\n",
      "  'precision', 'predicted', average, warn_for)\n"
     ]
    }
   ],
   "source": [
    "predictions = lsvc_model.predict(test_data)\n",
    "print(classification_report(test_target, predictions))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    " "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Feature Engineering"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Transforming categorical variables 'newDesk' and 'typeOfMaterial' using one-hot encoding"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from sklearn.preprocessing import OneHotEncoder, LabelEncoder\n",
    "filt = (comm.sectionName != \"Unknown\") & (comm.commentType == \"comment\")\n",
    "comm_filt = comm[filt].copy().reset_index()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# transform and map newDesk categories \n",
    "newDesk_le = LabelEncoder()\n",
    "newDesk_labels = newDesk_le.fit_transform(comm_filt['newDesk'])\n",
    "comm_filt['newDesk_label'] = newDesk_labels\n",
    "\n",
    "# encoding newDesk\n",
    "newDesk_ohe = OneHotEncoder()\n",
    "newDesk_feature_arr = newDesk_ohe.fit_transform(comm_filt[['newDesk_label']]).toarray()\n",
    "newDesk_feature_labels = list(newDesk_le.classes_)\n",
    "newDesk_features = pd.DataFrame(newDesk_feature_arr, columns=newDesk_feature_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/anaconda3/envs/ml/lib/python3.6/site-packages/sklearn/preprocessing/_encoders.py:363: FutureWarning: The handling of integer data will change in version 0.22. Currently, the categories are determined based on the range [0, max(values)], while in the future they will be determined based on the unique values.\n",
      "If you want the future behaviour and silence this warning, you can specify \"categories='auto'\".\n",
      "In case you used a LabelEncoder before this OneHotEncoder to convert the categories to integers, then you can now use the OneHotEncoder directly.\n",
      "  warnings.warn(msg, FutureWarning)\n"
     ]
    }
   ],
   "source": [
    "# transform and map typeOfMaterial categories \n",
    "material_le = LabelEncoder()\n",
    "material_labels = material_le.fit_transform(comm_filt['typeOfMaterial'])\n",
    "comm_filt['material_label'] = material_labels\n",
    "\n",
    "# encoding typeOfMaterial\n",
    "material_ohe = OneHotEncoder()\n",
    "material_feature_arr = material_ohe.fit_transform(comm_filt[['material_label']]).toarray()\n",
    "material_feature_labels = list(material_le.classes_)\n",
    "material_features = pd.DataFrame(material_feature_arr, columns=material_feature_labels)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "# subsetting comment data frame and concatenating with new features\n",
    "comm_filt_sub = comm_filt.loc[:,['articleID','commentID', 'commentBody', 'sectionName']]\n",
    "comm_df_ohe = pd.concat([comm_filt_sub, newDesk_features, material_features], axis=1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>commentBody</th>\n",
       "      <th>sectionName</th>\n",
       "      <th>Arts&amp;Leisure</th>\n",
       "      <th>BookReview</th>\n",
       "      <th>Business</th>\n",
       "      <th>Culture</th>\n",
       "      <th>Dining</th>\n",
       "      <th>Editorial</th>\n",
       "      <th>Express</th>\n",
       "      <th>Foreign</th>\n",
       "      <th>...</th>\n",
       "      <th>Weekend</th>\n",
       "      <th>Well</th>\n",
       "      <th>Editorial</th>\n",
       "      <th>News</th>\n",
       "      <th>News Analysis</th>\n",
       "      <th>Obituary (Obit)</th>\n",
       "      <th>Op-Ed</th>\n",
       "      <th>Question</th>\n",
       "      <th>Review</th>\n",
       "      <th>briefing</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>How could the league possibly refuse this offe...</td>\n",
       "      <td>Pro Football</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>So then the execs can be like \"yeah...we will ...</td>\n",
       "      <td>Pro Football</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>I would not want to play chess against these c...</td>\n",
       "      <td>Pro Football</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>Could the cheerleaders join the Actors' Equity...</td>\n",
       "      <td>Pro Football</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>The Iran nuclear deal does nothing but postpon...</td>\n",
       "      <td>Europe</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>...</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.0</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows × 32 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "                                         commentBody   sectionName  \\\n",
       "0  How could the league possibly refuse this offe...  Pro Football   \n",
       "1  So then the execs can be like \"yeah...we will ...  Pro Football   \n",
       "2  I would not want to play chess against these c...  Pro Football   \n",
       "3  Could the cheerleaders join the Actors' Equity...  Pro Football   \n",
       "4  The Iran nuclear deal does nothing but postpon...        Europe   \n",
       "\n",
       "   Arts&Leisure  BookReview  Business  Culture  Dining  Editorial  Express  \\\n",
       "0           0.0         0.0       0.0      0.0     0.0        0.0      0.0   \n",
       "1           0.0         0.0       0.0      0.0     0.0        0.0      0.0   \n",
       "2           0.0         0.0       0.0      0.0     0.0        0.0      0.0   \n",
       "3           0.0         0.0       0.0      0.0     0.0        0.0      0.0   \n",
       "4           0.0         0.0       0.0      0.0     0.0        0.0      0.0   \n",
       "\n",
       "   Foreign    ...     Weekend  Well  Editorial  News  News Analysis  \\\n",
       "0      0.0    ...         0.0   0.0        0.0   1.0            0.0   \n",
       "1      0.0    ...         0.0   0.0        0.0   1.0            0.0   \n",
       "2      0.0    ...         0.0   0.0        0.0   1.0            0.0   \n",
       "3      0.0    ...         0.0   0.0        0.0   1.0            0.0   \n",
       "4      0.0    ...         0.0   0.0        0.0   1.0            0.0   \n",
       "\n",
       "   Obituary (Obit)  Op-Ed  Question  Review  briefing  \n",
       "0              0.0    0.0       0.0     0.0       0.0  \n",
       "1              0.0    0.0       0.0     0.0       0.0  \n",
       "2              0.0    0.0       0.0     0.0       0.0  \n",
       "3              0.0    0.0       0.0     0.0       0.0  \n",
       "4              0.0    0.0       0.0     0.0       0.0  \n",
       "\n",
       "[5 rows x 32 columns]"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "comm_df_ohe.head()"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
